# NLP-Final-Project


State-of-the-art Transformer-based pre-trained natural language processing (NLP) models have largely focused on the English language. Models such as BERT, BART and GPT-3 have achieved state-of-the-art performance on many downstream tasks such as name-entity recognition and text summarization. However, these models are not optimized for less common languages such as Modern Greek. In 2021, Koutsikakis et al. presented GREEK-BERT as a monolingual modern GREEK NLP model based on Google's BERT. \cite{Koutsikakis} Koutsikakis et al. evaluated its performance on three downstream tasks: Part-of-speech tagging (POS tagging), named-entity recognition (NER),and natual language inference and found that the model outperformed multilingual Transformer based models such as M-BERT and XLM-R and other baseline models. In this project, we reproduce the results of this paper on the downstream task of named-entity recognition, and we also measure the effect of reducing the size of the dataset. We discover that a minimum of 600 training sentences are necessary to observe similar classification performance as GREEK-BERT's authors achieved on NER, implying that fine-tuning in Modern Greek can be much more data efficient than the authors initially hypothesized. 
